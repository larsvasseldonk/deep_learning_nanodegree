{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ccbc8d-6852-4114-8bc4-009a69a52a8c",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d88153-f889-47ad-a3e6-2e7848762eb0",
   "metadata": {},
   "source": [
    "Autoencoders have a similar backbone (called encoder in this context) that produces a feature vector (called embedding in this context). However, they substitute the fully-connected layers (the head) with a decoder stage whose scope is to reconstruct the input image starting from the embeddings.\n",
    "\n",
    "Uses of autoencoders:\n",
    "- Compress data\n",
    "- Denoise data\n",
    "- Find outliers (do anomaly detection) in a dataset\n",
    "- Do inpainting (i.e., reconstruct missing areas of an image or a vector)\n",
    "- With some modifications, we can use autoencoders as generative models - models capable of generating new images\n",
    "\n",
    "Autoencoders are a form of **unsupervised learning**, since we learn without having a label.\n",
    "\n",
    "The loss function for autoencoders is the MSE between each pixel between the input and output image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e635c-8801-4001-82c8-249bb6f05239",
   "metadata": {},
   "source": [
    "## Linear autoencoders\n",
    "\n",
    "The encoder and decoder are made of simple Multi-Layer Perceptrons. The units that connect the encoder and decoder will be the _compressed representation_ (also called _embedding_).\n",
    "\n",
    "Since the images are normalized between 0 and 1, you will need to use a **sigmoid activation on the output layer** to get values that match this input value range.\n",
    "\n",
    "Example of very simple autoencoder with two linear layers:\n",
    "```python\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, encoding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(encoding_dim)\n",
    "        )\n",
    "\n",
    "        ## decoder ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.auto_encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "\n",
    "        encoded = self.auto_encoder(x)\n",
    "\n",
    "        # Reshape the output as an image\n",
    "        # remember that the shape should be (batch_size, channel_count, height, width)\n",
    "        return encoded.reshape((x.shape[0], 1, 28, 28))\n",
    "```\n",
    "\n",
    "The main parts of autoencoders:\n",
    "- the encoder = takes the input image and encodes into a 1d vector (the embedding),\n",
    "- the decoder = takes the embedding and generates an image from it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0064a7-ed3b-40d2-a239-9335dbe96748",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "An anomaly is a data element that is an outlier with respect to the rest of the dataset.\n",
    "\n",
    "How autoencoders work for anomaly detection:\n",
    "\n",
    "*Autoencoders compress the visual information contained in images into a compact, latent representation (the embedding) that has a much lower dimensionality than the input image. By asking the decoder to reconstruct the input from this compact representation, we force the network to learn an embedding that stores meaningful information about the content of the image. For example, in the solution I compressed 28 x 28 images (so 784 pixels) into a vector of only 32 elements, but I was still able to reconstruct most of the images very well.*\n",
    "\n",
    "*When applying it to a test set that the network has never seen, most images were reconstructed well, but some of them were not. This means that the compression that the network has learned on the training dataset works well for the vast majority of the examples in this new set, but not for these anomalous ones. These anomalies have characteristics that the network is not well equipped to reconstruct, and therefore the decoder cannot recreate them faithfully during decoding.*\n",
    "\n",
    "*Through scoring each example by the loss, we are able to identify anomalies by simply taking the examples with the highest loss.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c99a1-41ff-41b6-a1a0-6e37e074220b",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "= Resizing an image to increase its size\n",
    "\n",
    "Instead of using linear layers to decode the image, we could also upsample a compact representation (embedding) into a full resolution image. For example, we could use a Transposed Convolutional Layer, which can learn how to best upsample an image.\n",
    "\n",
    "Upsampling techniques:\n",
    "- Transposed convolution: A layer that intelligently upsample an image, by using a learnable convolutional kernel\n",
    "- Nearest Neighbors upsampling: An upsampling technique that copies the value from the nearest pixel\n",
    "\n",
    "### Transposed Convolutional Layer\n",
    "\n",
    "Transposed Convolution can perform an upsampling of the input with learned weights. In particular, a Transposed Convolution with a 2 x 2 filter and a stride of 2 will double the size of the input image.\n",
    "\n",
    "Whereas a Max Pooling operation with a 2 x 2 window and a stride of 2 reduces the input size by half, a Transposed Convolution with a 2 x 2 filter and a stride of 2 will double the input size.\n",
    "\n",
    "Example code in PyTorch:\n",
    "```python\n",
    "unpool = nn.ConvTranspose2d(input_ch, output_ch, kernel_size, stride=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9430e61-596a-4ac7-a966-96dd4679b522",
   "metadata": {},
   "source": [
    "## Convolutional Autoencoder\n",
    "\n",
    "Example code in PyTorch:\n",
    "```python\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        ## encoder ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        ## decoder ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Undo the Max Pooling\n",
    "            nn.ConvTranspose2d(3, 1, 2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.auto_encoder = nn.Sequential(\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "\n",
    "        return self.auto_encoder(x)\n",
    "```\n",
    "\n",
    "Note that:\n",
    "- we don't have to flatten the image (like with the linear autoencoder) since we're using a convolutional layer (we keep spatial information)\n",
    "- the `ConvTranspose2d` basically undo's the `MaxPool2d` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd83775-25e2-40bc-b871-a9fdf1be74ee",
   "metadata": {},
   "source": [
    "## Denoising\n",
    "\n",
    "= the task of removing noise from an image by reconstructing a denoised image.\n",
    "\n",
    "A denoising autoencoder is a normal autoencoder, but trained in a specific way.\n",
    "\n",
    "Operations of training a denoising autoencoder:\n",
    "- loop over each batch in the training data loader\n",
    "- add noise to the images in the batch\n",
    "- compute the prediction from the network, i.e. the reconstructed images\n",
    "- compare the reconstructed images with the input (uncorrupted) images using a loss like `nn.MSELoss()`\n",
    "- perform backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e92126-0ebe-4132-96f4-46e4c08abbc8",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "Autoencoder: A neural network architecture consisting of an encoder part, which takes an input and compresses it into a low-dimensional embedding vector, and a decoder part, which takes the embedding and tries to reconstruct the input image from it.\n",
    "\n",
    "Transposed Convolution: A special type of convolution that can be used to intelligently upsample an image or a feature map\n",
    "\n",
    "Denoising: The task of taking an image corrupted by noise and generating a version of the image where the noise has been removed.\n",
    "\n",
    "Variational autoencoder (VAE): An extension of the idea of autoencoders that transforms them into proper generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbea0b2-adf0-46a1-a3ef-053324832d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
