{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aeed9b8-5975-4efd-88bf-a5dcf0614331",
   "metadata": {},
   "source": [
    "# Object detection and segmentation\n",
    "\n",
    "Different types of tasks for computer vision:\n",
    "- Image classification: Assign one or more labels to an image\n",
    "- **Object localization**: Assign a label to most prominent object, define a box around that object\n",
    "- **Object detection**: Assign a label and define a box for all objects in an image\n",
    "- **Semantic segmentation**: Determine the class of each pixel in the image\n",
    "- Instance segmentation: Determine the class of each pixel in the image distinguishing different instances of the same class\n",
    "\n",
    "In this lesson, we focus on the bold types of tasks.\n",
    "\n",
    "Input vs. tasks:\n",
    "- Image label(s) -> Image classification\n",
    "- Object label, bounding box of the object of interest -> Object localization\n",
    "- Object labels, bounding boxes of all objects of interest -> Object detection\n",
    "- Semantic mask with foreground and background -> Binary semantic segmentation\n",
    "- Semantic masks for all classes (including background) -> Multi-class semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0722c1-7f2a-4203-8c77-77707274799a",
   "metadata": {},
   "source": [
    "## Object localization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56ee96-bfb7-4896-9794-70a47435c579",
   "metadata": {},
   "source": [
    "\n",
    "Definitions:\n",
    "- Object localization = the task of assigning a label and determining the bounding box of an object of interest in an image.\n",
    "- A bounding box = a rectangular box that completely encloses the object, whose sides are parallel to the sides of the image.\n",
    "- **Multi-head model** = a CNN where we have a backbone as typical for CNNs, but more than one head.\n",
    "\n",
    "Example of Multi-Head model in PyTorch:\n",
    "```python\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Backbone: this can be a custom network, or a\n",
    "        # pre-trained network such as a resnet50 where the\n",
    "        # original classification head has been removed. It computes\n",
    "        # an embedding for the input image\n",
    "        self.backbone = nn.Sequential(..., nn.Flatten())\n",
    "\n",
    "        # Classification head: an MLP or some other neural network\n",
    "        # ending with a fully-connected layer with an output vector\n",
    "        # of size n_classes\n",
    "        self.class_head = nn.Sequential(..., nn.Linear(out_feature, n_classes))\n",
    "\n",
    "        # Localization head: an MLP or some other neural network\n",
    "        # ending with a fully-connected layer with an output vector\n",
    "        # of size 4 (the numbers defining the bounding box)\n",
    "        self.loc_head = nn.Sequential(..., nn.Linear(out_feature, 4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "\n",
    "        class_scores = self.class_head(x)\n",
    "        bounding_box = self.loc_head(x)\n",
    "\n",
    "        return class_scores, bounding_box\n",
    "```\n",
    "\n",
    "When having a Multi-Head model we need to combine loss functions (the one for the class, and the one for the bounding box). In PyTorch we can implement the multiple losses the following way:\n",
    "\n",
    "```python\n",
    "class_loss = nn.CrossEntropyLoss()\n",
    "loc_loss = nn.MSELoss()\n",
    "alpha = 0.5\n",
    "\n",
    "...\n",
    "for images, labels in train_data_loader:\n",
    "    ...\n",
    "\n",
    "    # Get predictions\n",
    "    class_scores, bounding_box = model(images)\n",
    "\n",
    "    # Compute sum of the losses\n",
    "    loss = class_loss(class_scores) + alpha * loc_loss(bounding_box)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f97c7e-bf08-4340-a501-a3d6d14bf1bf",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "The task of object detection consists of detecting and localizing all the instances of the objects of interest.\n",
    "\n",
    "Nowadays there are two approaches to solving the problem of handling a variable number of objects, and of their different aspect ratios and scales:\n",
    "\n",
    "### One-stage object detection\n",
    "We consider a fixed number of windows with different scales and aspect ratios, centered at fixed locations (anchors). The output of the network then has a fixed size. The localization head will output a vector with a size of 4 times the number of anchors, while the classification head will output a vector with a size equal to the number of anchors multiplied by the number of classes.\n",
    "\n",
    "### Two-stage object detection\n",
    "In the first stage, an algorithm or a neural network proposes a fixed number of windows in the image. These are the places in the image with the highest likelihood of containing objects. Then, the second stage considers those and applies object localization, returning for each place the class scores and the bounding box.\n",
    "\n",
    "In practice, the difference between the two is that while the first type has fixed anchors (fixed windows in fixed places), the second one optimizes the windows based on the content of the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c7f2b-c060-4bc3-8fdc-a05dd259d617",
   "metadata": {},
   "source": [
    "## One-Stage Object Detection: RetinaNet\n",
    "\n",
    "We divide the image with a regular grid. Then for each grid cell we consider a certain number of windows with different aspect ratios and different sizes. We then \"anchor\" the windows in the center of each cell. If we have 4 windows and 45 cells, then we have 180 anchors.\n",
    "\n",
    "Summarizing, RetinaNet is characterized by three key features:\n",
    "- Anchors - Anchors are windows with different sizes and different aspect ratios, placed in the center of cells defined by a grid on the image.\n",
    "- Feature Pyramid Networks - The Feature Pyramid Network(opens in a new tab) is an architecture that extracts multi-level, semantically-rich feature maps from an image:\n",
    "- Focal loss - The Focal Loss adds a factor in front of the normal cross-entropy loss to dampen the loss due to examples that are already well-classified so that they do not dominate. This factor introduces a hyperparameter γ: the larger γ, the more the loss of well-classified examples is suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514f6d8-c898-4d7b-b23b-f9e8ee97dc7a",
   "metadata": {},
   "source": [
    "## Object detection metrics\n",
    "\n",
    "The following metrics are often used:\n",
    "- **Intersection over Union** - The IoU is a measure of how much two boxes (or other polygons) coincide. As the name suggests, it is the ratio between the area of the intersection, or overlap, and the area of the union of the two boxes or polygons\n",
    "- **Average Precision** - Area under the precision-recall curve\n",
    "- **Mean Average Precision (mAP)** - Average over all classes of the Average Precision\n",
    "- **Average Recall** - Area under the recall vs IoU curve\n",
    "- **Mean Average Recall (mAR)** - Average over all classes of the Average Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1dc5f-dce9-4466-bebd-83717fdfaaa6",
   "metadata": {},
   "source": [
    "## Semantic segmentation \n",
    "\n",
    "Semantic segmentation is one key technique of image segmentation.\n",
    "\n",
    "The task of semantic segmentation consists of classifying each pixel of an image to determine to which class it belongs.\n",
    "\n",
    "### UNet architecture\n",
    "The UNet is a specific architecture for semantic segmentation. It has the structure of a standard autoencoder, with an encoder that takes the input image and encodes it through a series of convolutional and pooling layers into a low-dimensional representation.\n",
    "\n",
    "Then the decoder architecture starts from the same representation and constructs the output mask by using transposed convolutions. However, the UNet adds skip connections between the feature maps at the same level in the encoder and in the decoder.\n",
    "\n",
    "Example in PyTorch:\n",
    "```python\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Binary segmentation?\n",
    "binary = True\n",
    "n_classes = 1\n",
    "\n",
    "model = smp.Unet(\n",
    "        encoder_name='resnet50',\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=3,\n",
    "        # +1 is for the background\n",
    "        classes=n_classes if binary else n_classes + 1)\n",
    "```\n",
    "\n",
    "### Dice loss\n",
    "The Dice Loss function is often used for semantic segmentation. The Dice loss derives from the F1 score, which is the geometric mean of precision and recall. Consequently, the Dice loss tends to balance precision and recall at the pixel level. A perfect Dice score is 0.\n",
    "\n",
    "In PyTorch:\n",
    "```python\n",
    "loss = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a54d3a-d4f2-43d6-8cce-0f8100a0c86b",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "Object localization: The task of determining if an image contains an object, and localize it with a bounding box.\n",
    "\n",
    "Bounding box: A rectangular box that completely encloses a given object in an image, whose sides are parallel to the sides of the image.\n",
    "\n",
    "Multi-head model: A CNN where we have one backbone but more than one head.\n",
    "\n",
    "Object detection: The task of localizing using a bounding box every object of interest in an image.\n",
    "\n",
    "Anchors: Windows with different sizes and different aspect ratios, placed in the center of cells defined by a grid on an image.\n",
    "\n",
    "Feature Pyramid Network (FPN): An architecture that extracts multi-level, semantically-rich feature maps from an image.\n",
    "\n",
    "Focal Loss: A modification of the Cross-Entropy Loss, **by **including a factor in front of the CE Loss to dampen the loss due to examples that are already well-classified, so they do not dominate.\n",
    "\n",
    "Mean Average Recall (mAR): A metric for object detection algorithms. It is obtained by computing the Average Recall for each class of objects, as twice the integral of the Recall vs IoU curve, and then by averaging the Average Recall for each class.\n",
    "\n",
    "Mean Average Precision (mAP): A metric for object detection algorithms. It is obtained by computing the Average Precision (AP) for each class. The AP is computed by integrating an interpolation of the Precision-Recall curve. The mAP is the mean average of the AP over the classes.\n",
    "\n",
    "Intersection over Union (IoU): The ratio between the area of the intersection, or overlap, and the area of the union of two boxes or polygons. Used to measure how much two boxes coincide.\n",
    "\n",
    "Semantic segmentation: The task of assigning a class to each pixel in an image.\n",
    "\n",
    "Dice loss: A useful measure of loss for semantic segmentation derived from the F1 score, which is the geometric mean of precision and recall. The Dice loss tends to balance precision and recall at the pixel level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8485c3e-4965-435b-913c-09d6eb35c1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
