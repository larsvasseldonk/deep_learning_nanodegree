{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a4ad11-e9a4-4f2e-944c-f9a05490b4bf",
   "metadata": {},
   "source": [
    "# CNNs in Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb26fc-cc42-4b54-82e6-f8c86ca19a9b",
   "metadata": {},
   "source": [
    "## Convolutional layers in depth\n",
    "\n",
    "Definitions:\n",
    "- dense layers = fully connected layers, meaning that the nodes are connected to every node in the previous layer\n",
    "- locally-connected layers (convolutional layers) = meaning that the nodes are only connected to a small subset of the previous layer nodes\n",
    "\n",
    "Kernels for color images:\n",
    "- grayscale images uses a matrix of `kernel_size` x `kernel_size` numbers\n",
    "- color images uses a 3d filter of `kernel_size` x `kernal_size` x 3 for RGB colors\n",
    "\n",
    "\"Channels\" indicate the feature maps of convolutional layers: a convolutional layer that takes feature maps with a depth of 64 and outputs 128 feature maps is said to have 64 channels as input and 128 as outputs.\n",
    "\n",
    "### The number of parameters in a convolutional layer\n",
    "The number of parameters in a convolutional layer is the number of filters times the number of input feature maps times the size of the kernel squared plus the number of kernels:\n",
    "- $n_k$​ : number of filters in the convolutional layer\n",
    "- $k$ : height and width of the convolutional kernel\n",
    "- $c$ : number of feature maps produced by the previous layer (or number of channels in input image)\n",
    "\n",
    "There are k times k times c weights per filter plus one bias per filter, so $c*k2 + 1$ parameters. The convolutional layer is composed of $n_k$​ filters, so the total number of parameters in the convolutional layer is: $n_p = n_k (c k^2 + 1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3758283-8dbf-4a9b-a728-f96ace78eb42",
   "metadata": {},
   "source": [
    "## Convolutional Layers in PyTorch\n",
    "\n",
    "Example code to create a convolutional layer:\n",
    "```python\n",
    "from torch import nn\n",
    "conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "```\n",
    "\n",
    "We can also add an activation and dropout function. In case of CNNs, we need to use the 2d version of dropout, which randomly drops some input channel entirely:\n",
    "```python\n",
    "conv_block = nn.Sequential(\n",
    "  nn.Conv2d(in_channels, out_channels, kernel_size),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout2d(p=0.2)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee69db-2438-4f2e-9992-888dba7bd820",
   "metadata": {},
   "source": [
    "## Stride and padding\n",
    "\n",
    "Padding and stride are hyperparameters, or configuration settings, for the filter:\n",
    "- Padding: Expanding the size of an image by adding pixels at its border\n",
    "- Stride: Amount by which a filter slides over an image.\n",
    "\n",
    "There are multiple padding strategies:\n",
    "- zero-padding strategy is by far the most common: padding values will all be 0\n",
    "- reflect: padding pixels filled with copies of values in input image taken in opposite order, in a mirroring fashion\n",
    "- replicate: padding pixels filled with value of closest pixel in input image\n",
    "- circular: like reflect mode, but image is first flipped horizontally and vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab72854-c8e9-4003-b757-c6ae2f1070af",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "Also for pooling layers you can set the padding and stride paramters.\n",
    "\n",
    "Different padding strategies:\n",
    "- max-pooling: take the maximum value in the window = mostly used for images\n",
    "- average-pooling: take mean average of all the values in the window\n",
    "\n",
    "Example code for max and average pooling:\n",
    "```python\n",
    "from torch import nn\n",
    "nn.MaxPool2d(kernel_size, stride)\n",
    "nn.AvgPool2d(window_size, stride)\n",
    "```\n",
    "\n",
    "Definitions:\n",
    "- Kerel size: size of the side of the convolutional kernel\n",
    "- Window size: size of the window considered during pooling\n",
    "- Stride: Step size of the convolutional kernel or of the pooling window when moving over the input image\n",
    "- Padding: Border to add to an input image before the convolution operation is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd1ce5-4492-44de-a187-c764cf270df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
