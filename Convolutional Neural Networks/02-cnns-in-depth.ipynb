{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a4ad11-e9a4-4f2e-944c-f9a05490b4bf",
   "metadata": {},
   "source": [
    "# CNNs in Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb26fc-cc42-4b54-82e6-f8c86ca19a9b",
   "metadata": {},
   "source": [
    "## Convolutional layers in depth\n",
    "\n",
    "Definitions:\n",
    "- dense layers = fully connected layers, meaning that the nodes are connected to every node in the previous layer\n",
    "- locally-connected layers (convolutional layers) = meaning that the nodes are only connected to a small subset of the previous layer nodes\n",
    "\n",
    "Kernels for color images:\n",
    "- grayscale images uses a matrix of `kernel_size` x `kernel_size` numbers\n",
    "- color images uses a 3d filter of `kernel_size` x `kernal_size` x 3 for RGB colors\n",
    "\n",
    "\"Channels\" indicate the feature maps of convolutional layers: a convolutional layer that takes feature maps with a depth of 64 and outputs 128 feature maps is said to have 64 channels as input and 128 as outputs.\n",
    "\n",
    "### The number of parameters in a convolutional layer\n",
    "The number of parameters in a convolutional layer is the number of filters times the number of input feature maps times the size of the kernel squared plus the number of kernels:\n",
    "- $n_k$​ : number of filters in the convolutional layer\n",
    "- $k$ : height and width of the convolutional kernel\n",
    "- $c$ : number of feature maps produced by the previous layer (or number of channels in input image)\n",
    "\n",
    "There are k times k times c weights per filter plus one bias per filter, so $c*k2 + 1$ parameters. The convolutional layer is composed of $n_k$​ filters, so the total number of parameters in the convolutional layer is: $n_p = n_k (c k^2 + 1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3758283-8dbf-4a9b-a728-f96ace78eb42",
   "metadata": {},
   "source": [
    "## Convolutional Layers in PyTorch\n",
    "\n",
    "Example code to create a convolutional layer:\n",
    "```python\n",
    "from torch import nn\n",
    "conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "```\n",
    "\n",
    "We can also add an activation and dropout function. In case of CNNs, we need to use the 2d version of dropout, which randomly drops some input channel entirely:\n",
    "```python\n",
    "conv_block = nn.Sequential(\n",
    "  nn.Conv2d(in_channels, out_channels, kernel_size),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout2d(p=0.2)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee69db-2438-4f2e-9992-888dba7bd820",
   "metadata": {},
   "source": [
    "## Stride and padding\n",
    "\n",
    "Padding and stride are hyperparameters, or configuration settings, for the filter:\n",
    "- Padding: Expanding the size of an image by adding pixels at its border\n",
    "- Stride: Amount by which a filter slides over an image.\n",
    "\n",
    "There are multiple padding strategies:\n",
    "- zero-padding strategy is by far the most common: padding values will all be 0\n",
    "- reflect: padding pixels filled with copies of values in input image taken in opposite order, in a mirroring fashion\n",
    "- replicate: padding pixels filled with value of closest pixel in input image\n",
    "- circular: like reflect mode, but image is first flipped horizontally and vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab72854-c8e9-4003-b757-c6ae2f1070af",
   "metadata": {},
   "source": [
    "## Pooling layers\n",
    "\n",
    "Also for pooling layers you can set the padding and stride paramters.\n",
    "\n",
    "Different padding strategies:\n",
    "- max-pooling: take the maximum value in the window = mostly used for images\n",
    "- average-pooling: take mean average of all the values in the window\n",
    "\n",
    "Example code for max and average pooling:\n",
    "```python\n",
    "from torch import nn\n",
    "nn.MaxPool2d(kernel_size, stride)\n",
    "nn.AvgPool2d(window_size, stride)\n",
    "```\n",
    "\n",
    "Definitions:\n",
    "- Kerel size: size of the side of the convolutional kernel\n",
    "- Window size: size of the window considered during pooling\n",
    "- Stride: Step size of the convolutional kernel or of the pooling window when moving over the input image\n",
    "- Padding: Border to add to an input image before the convolution operation is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72b72b-d1b8-4cef-adf2-8a1b35738998",
   "metadata": {},
   "source": [
    "## Typical structure of CNNs\n",
    "\n",
    "Example code of typical convolutional layer:\n",
    "```python\n",
    "conv1 = nn.Conv2d(\n",
    "    depth_of_input_layer, # for an RBG image, the input size is 3\n",
    "    desired_depth_of_output, # if we want 16 filters, this will be 16\n",
    "    kernel_size, # size of the filter\n",
    "    stride, # default is 1\n",
    "    padding, # default is 0\n",
    ")\n",
    "```\n",
    "\n",
    "A classical CNN is made of two distinct parts, sometimes called the backbone and the head:\n",
    "- The **backbone** is made of convolutional and pooling layers, and has the task of extracting information from the image.\n",
    "- After the backbone there is a flattening layer that takes the output feature maps of the previous convolutional layer and flattens them out in a 1d vector: for each feature map the rows are stacked together in a 1d vector, then all the 1d vectors are stacked together to form a long 1d vector called a feature vector or embedding.\n",
    "\n",
    "### A simple CNN\n",
    "\n",
    "Example of a simple CNN in Pytorch:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    # Create layers. In this case just a standard MLP\n",
    "    self.model = nn.Sequential(\n",
    "      # First conv + maxpool + relu\n",
    "      nn.Conv2d(3, 16, 3, padding=1),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout2d(0.2),\n",
    "\n",
    "      # Second conv + maxpool + relu\n",
    "      nn.Conv2d(16, 32, 3, padding=1),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout2d(0.2),\n",
    "\n",
    "      # Third conv + maxpool + relu\n",
    "      nn.Conv2d(32, 64, 3, padding=1),\n",
    "      nn.MaxPool2d(2, 2),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout2d(0.2),\n",
    "\n",
    "      # Flatten feature maps\n",
    "      nn.Flatten(),\n",
    "\n",
    "      # Fully connected layers. This assumes\n",
    "      # that the input image was 32x32\n",
    "      nn.Linear(1024, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(128, n_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # nn.Sequential will call the layers \n",
    "    # in the order they have been inserted\n",
    "    return self.model(x)\n",
    "```\n",
    "\n",
    "### Output volume for a convolutional layer\n",
    "\n",
    "To compute the output size of a given convolutional layer we can perform the following calculation (taken from [Stanford's cs231n course](http://cs231n.github.io/convolutional-networks/#layers)):\n",
    "- We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by: $\\frac{{\\text{{Input size W}} - \\text{{Kernel size F}} + 2 \\times \\text{{Padding P}}}}{{\\text{{Stride S}}}} + 1$\n",
    "\n",
    "\n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output:\n",
    "- W = 7\n",
    "- F = 3\n",
    "- S = 1\n",
    "- P = 0\n",
    "- ((7-3+2*0)/1) + 1 = 5\n",
    "\n",
    "With stride 2 we would get a 3x3 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92367e6b-de19-45c4-bfc2-b3b01685e4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85867f0-d722-45db-8781-dec306ae1eda",
   "metadata": {},
   "source": [
    "# Misc\n",
    "\n",
    "A tensor can be moved to GPU or CPU by:\n",
    "```python\n",
    "tensor.cuda()\n",
    "tensor.cpu()\n",
    "```\n",
    "\n",
    "CUDA is a parallel computing platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd846fd-7742-490c-b559-ecfb8d41b1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
