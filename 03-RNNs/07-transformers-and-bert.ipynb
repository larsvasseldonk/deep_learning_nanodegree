{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20febe7d-6082-4361-abbf-21c1e5deb277",
   "metadata": {},
   "source": [
    "# Behond RNNs: Transformers and BERT\n",
    "\n",
    "In this lesson, we will cover the following topics:\n",
    "- The Evolution of Natural Language Processing\n",
    "- Benefits of Transformer Architectures\n",
    "- BERT, GPT-3, and Their Capabilities\n",
    "- New Ways of Evaluating Our Models\n",
    "\n",
    "## Evolution of NLP\n",
    "- data scientist experimented to improve RNNs and discovered a new technique called \"Attention\"\n",
    "- the goal of Attention is to \"focus\" the model on certain parts of the data\n",
    "- this technique showed benefit for RNNs and architectures like Seq2Seq\n",
    "- a landmark paper was released by Google demonstrating that attention inside an autoencoder architecture was superior to recurrents in RNNs\n",
    "\n",
    "Architecture of Transfomer:\n",
    "- The Transformer keeps the Encoder and Decoder from Seq2Seq\n",
    "- The Encoder takes input embeddings and passes them through an attention-feed forward NN loop N times\n",
    "- Then, it passes the output to the Decoder where it passes through another attention-feed forward NN loop\n",
    "- Output then goes through a Linear and Softmax layer\n",
    "\n",
    "Transformer architectures do not use recurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f43f8a-d597-4dfc-b9af-68b170642d87",
   "metadata": {},
   "source": [
    "## Benefits of Transformer Architectures\n",
    "\n",
    "### Faster to Train\n",
    "The replacement of recurrent cells with feedforward networks improves the parallelization of Transformers. Current high-performance computing systems are designed to work well with this type of parallelization.\n",
    "\n",
    "### Better Performance\n",
    "Transformers offer better performance than RNNs across most natural language tasks. Therefore, we can use them to solve new problems.\n",
    "\n",
    "### Versatility\n",
    "The Transformer architecture can move between different domains like NLP and Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81366fc-3931-4ed7-9e06-00ee93bcf860",
   "metadata": {},
   "source": [
    "## Difference between RNNs and Transformers\n",
    "\n",
    "Udacity answer:\n",
    ">RNNs use a concept called “memory” to learn underlying patterns in a sequence and project them forward. They can do this inside of a greater patter or outside of a greater pattern. In sequence-to-sequence architectures, we learned that an RNN can fit within an encoder-decoder construct to solve NLP problems. Transformers are reliant on the encoder-decoder pattern to solve problems. Specifically, Tranformers remove the RNN entirely and rely on ever greater stacks of encoder-decoders to solve their problem.\n",
    "\n",
    "My answer:\n",
    ">The most important difference between Transformers and RNNs is the fact that Transformers do not rely on recurrence and therefore don't use the concept of \"memory\" like RNN uses. Instead, Transformers introduce a new technique called Attention to \"focus\" the model on certain parts of the data. The replacement of recurrent cells with feedforward networks improves the speed of training. It also has shown better performance than RNNs across most natural language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d60348-2c8e-4d7e-991d-694aea5049e2",
   "metadata": {},
   "source": [
    "## Example code using BERT\n",
    "\n",
    "Using BERT with Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a12f675-162c-407b-b961-fa1429aa3c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.911424458026886,\n",
       " 'start': 0,\n",
       " 'end': 17,\n",
       " 'answer': 'The Great Pumpkin'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the BERT model using the pipeline function, define a compatible model and tokenize\n",
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\") # Question-answering pipeline defaults to DistilBERT\n",
    "\n",
    "## Provide question and context\n",
    "\n",
    "question = \"Who is the Great Pumpkin?\"\n",
    "context = '''The Great Pumpkin is a supernatural figure who rises from the pumpkin patch on Halloween evening, and flies around bringing toys to sincere and believing children.'''\n",
    "\n",
    "## Ask question\n",
    "\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd9948-51ce-41a4-9067-922627abe998",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "BERT is a state-of-the-art Transformer released by Google in 2018. BERT pioneered several techniques to achieve cutting-edge results.\n",
    "\n",
    "First, BERT is pretrained on a natural language corpus to develop a general understanding of a language. Then, the model is fine-tuned to perform specific tasks.\n",
    "\n",
    "Next, BERT used bidirectional embeddings to capture relationships on both sides of a given word. This improved performance over unidirectional embeddings.\n",
    "\n",
    "Finally, BERT stacked encoders and decoders together. The stacked encoder architecture increased the size of the model and dramatically increased the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13669cb7-f0e3-48b7-a1d7-331b1fe7d543",
   "metadata": {},
   "source": [
    "## GLUE\n",
    "GLUE stands for General Language Understanding Evaluation. This is a new metric to measure a general understanding of language. It measures 9 language tasks. \n",
    "\n",
    "See http://gluebenchmark.com\n",
    "\n",
    "Other metrics:\n",
    "- BLEU is a specialized metric for language translation!\n",
    "- ROUGE is a metric for text summarization. This is a new capability of transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233aa600-81e7-4d31-b516-8b30dd6b7c5d",
   "metadata": {},
   "source": [
    "## Future of NLP\n",
    "\n",
    "Research Areas\n",
    "- **Types of Attention**: Attention is at the forefront of these new models. Modifications to the attention mechanism may improve modeling results.\n",
    "- **Few-shot Learning**: Some problems don't have enough data to be solved with current architectures. Streamlining architectures to learn from a few examples will open up new application areas.\n",
    "- **Combining Multiple Tasks**: Developing models that can perform multiple tasks may lead to the development of general intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c0f45-f397-4183-a70c-5e09021ff5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
